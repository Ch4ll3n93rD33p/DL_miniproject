{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--Cvru1cgwyP"
   },
   "source": [
    "# **Miniproject 2**\n",
    "## **~Large~ Small Language Model**\n",
    "\n",
    "### **Objective**\n",
    "Implement a transformer-based, character-level language model (GPT-like) and train it on the Shakespeare dataset. By the end of this project, you should be able to generate Shakespearean-like text given a seed string.\n",
    "\n",
    "You will probably want to train the model on a GPU. You can use free GPUs on [Google Colab](https://colab.research.google.com/?utm_source=scs-index)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f_rT3xwrhieb"
   },
   "source": [
    "### **Dataset**:\n",
    "\n",
    "The Shakespeare dataset contains the complete works of William Shakespeare, including his plays, poems, and sonnets.\n",
    "\n",
    "[**Download link**](https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt)\n",
    "\n",
    "In a character-level language model, each character in the input data is mapped to its respective index from a dictionary. The input to the model is in the form (B, N), where B is the batch size and N is the number of tokens for each sequence. The model was tested with B=N=128, but feel free to explore different values.\n",
    "\n",
    "An interface for the dataset class that takes care of tokenization is provided below.\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits batches of characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config, data):\n",
    "\n",
    "        chars = ... # get characters from the input data\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) } # map characters to integer indices\n",
    "\n",
    "        ...\n",
    "\n",
    "    def get_vocab_size(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __len__(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        # encode every character to an integer\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        pass\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VV7OAXGRhf_V"
   },
   "source": [
    "### **Requirements**\n",
    "\n",
    "#### **Architecture**\n",
    "\n",
    "Implement the Transformer's decoder-only structure.\n",
    "This includes\n",
    "\n",
    "* input token embeddings\n",
    "* the causal multi-head self-attention mechanism\n",
    "* feed-forward neural networks\n",
    "* positional encodings, residual connections, layer normalizations.\n",
    "\n",
    "The project was tested with $12$ layers, $8$ attention heads, and $768$ embedding dimensions, on a single GPU.\n",
    "\n",
    "The `forward` method for the entire model has the following form:\n",
    "\n",
    "```\n",
    "tok_emb = WTE(idx) # token embeddings\n",
    "pos_emb = WPE(pos) # position embeddings\n",
    "x = Dropout(tok_emb + pos_emb)\n",
    "for Block in Blocks:\n",
    "    x = Block(x)\n",
    "x = Final_LayerNorm(x)\n",
    "logits = LM_Head(x)\n",
    "```\n",
    "\n",
    "The `forward` method for the transformer block has the following form:\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "out = x + self.MLP(self.LayerNorm_2(x))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### **Training**\n",
    "\n",
    "In a character-level transformer language model, the goal is to predict the next character in a sequence given the previous characters. To train such a model effectively, we use two versions of our data: the input sequence and a shifted version of this sequence, which serves as the target for our predictions.\n",
    "\n",
    "Preprocess the dataset to a character-level representation.\n",
    "Use a sliding window approach for sequence chunks (e.g., window size of $128$ characters).\n",
    "Implement causal masking for the self-attention mechanism.\n",
    "Use the [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) optimizer and the cross-entropy loss.\n",
    "\n",
    "**Optional**:\n",
    "\n",
    "* Implement a learning rate decay strategy\n",
    "* Implement gradient clipping\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "#### **Evaluation and Inference**\n",
    "\n",
    "* Monitor the cross-entropy loss. Use a seed string to initialize the model and generate Shakespearean-like text.\n",
    "\n",
    "* In order to generate the characters, at each generation step you can either select the character with the highest probability, or you can sample according to the output distribution.\n",
    "\n",
    "The high-level pseudocode for generation is:\n",
    "\n",
    "```python\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    context = \"O God, O God!\"\n",
    "    tokenized_context = tokenize(context)\n",
    "    # the model should implement a method to generate tokens given a prompt\n",
    "    y = model.generate(tokenized, ...)\n",
    "    completion = tokens_to_string(y)\n",
    "```\n",
    "\n",
    "**Optional**:\n",
    "* Compute the [perplexity](https://medium.com/@priyankads/perplexity-of-language-models-41160427ed72#:~:text=Intuitively%2C%20perplexity%20means%20to%20be,loss%20obtained%20from%20the%20model.) metric for quantitative evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t88Dcn8JZ8M"
   },
   "source": [
    "### **Example Outputs**\n",
    "\n",
    "The following are my outputs after $6000$ steps of training, with the seed string \"O God, O God!\"\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "O God, O God! neither? unto the base very ears,\n",
    "As damned with it.\n",
    "\n",
    "DUKE OF YORK:\n",
    "Away! Once more, one word.\n",
    "\n",
    "RICHARD:\n",
    "Clove, dear so; and therein my son will be\n",
    "false of woe: if ye seems to be the mother\n",
    "Of gracious order this time when R going kinsperse eyes,\n",
    "What dost bewreck her fairer drying tears.\n",
    "\n",
    "NORTHUMBERLAND:\n",
    "Have you forgot the Duke of Norfolk, get him to\n",
    "again; and and agilic: there is my spirit\n",
    "So maly did must such a marble perfection.\n",
    "\n",
    "ELBOW:\n",
    "Come, bring them with oaths, and so deliver\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k0SY7CGAhnkp"
   },
   "source": [
    "### Resources:\n",
    "\n",
    "* Vaswani et al., \"Attention is All You Need\": [link](https://arxiv.org/abs/1706.03762)\n",
    "\n",
    "* Illustrated Transformer by Jay Alammar: [link](https://jalammar.github.io/illustrated-transformer/)\n",
    "\n",
    "* OpenAI GPT-2 Paper: [link](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n",
    "\n",
    "* Deep Learning Course slides on transformers: [link](https://fleuret.org/dlc/materials/dlc-handout-13-3-transformers.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW3Mfl4t56jy"
   },
   "source": [
    "# _____________________________________________________________________________\n",
    "Lea Heiniger  \n",
    "December 2024  \n",
    "\n",
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dZdSRWPmgt-H",
    "outputId": "f259d5fd-7bc4-4d75-e93a-9f17c2dd4dcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "print(torch.cuda.is_available()) # test if cuda is usable\n",
    "\n",
    "# import input.txt\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hM3Je1CU56jy"
   },
   "source": [
    "## Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "8z-p1rSN56jy"
   },
   "outputs": [],
   "source": [
    "## Data and Tokenisation\n",
    "\n",
    "def get_data_from_file(filename : str) -> str :\n",
    "\n",
    "    ## Reads the Input File and Returns the Content as a String ##\n",
    "\n",
    "\n",
    "    with open(filename, 'r') as f :\n",
    "\n",
    "        data = f.read()\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Emits Batches of Characters.\n",
    "\n",
    "    Adapted from \"https://github.com/karpathy/minGPT\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config : dict, data : str) :\n",
    "\n",
    "        self.data = data\n",
    "        chars = sorted(set(data)) # get characters from the input data\n",
    "        self.stoi = {ch : i for i,ch in enumerate(chars)}  # map characters to integer indices\n",
    "        self.itos = {i : ch for i, ch in enumerate(chars)} # char to int (used for tokens_to_string())\n",
    "        self.batch_size = config['batch_size']\n",
    "\n",
    "    def get_vocab_size(self) -> int :\n",
    "\n",
    "        vocab_size = len(self.stoi)\n",
    "\n",
    "        return vocab_size\n",
    "\n",
    "    def __len__(self) -> int :\n",
    "\n",
    "        length = len(self.data)-self.batch_size\n",
    "\n",
    "        return length\n",
    "\n",
    "    def __getitem__(self, idx : int) -> tuple[torch.Tensor, torch.Tensor] :\n",
    "\n",
    "        # grab a chunk of (block_size + 1) characters from the data\n",
    "        char_chunk = self.data[idx:idx+self.batch_size+1]\n",
    "\n",
    "        # encode every character to an integer\n",
    "        integer_chunk = [self.stoi[ch] for ch in char_chunk]\n",
    "\n",
    "        # return the chunk and the shifted version as tensors\n",
    "        X = torch.tensor(integer_chunk[:-1])\n",
    "        Y = torch.tensor(integer_chunk[1:])\n",
    "\n",
    "        return X, Y\n",
    "\n",
    "    def tokenize(self, string: str) -> list[int] :\n",
    "\n",
    "        ## Tokenizes a String ##\n",
    "\n",
    "        # return tokenized as a list of tokens (int)\n",
    "        tokenized = [self.stoi[ch] for ch in string if ch in self.stoi]\n",
    "\n",
    "        return tokenized\n",
    "\n",
    "    def tokens_to_string(self, tokens: list[int]) -> str :\n",
    "\n",
    "        ## Convert the List of Tokens to a String ##\n",
    "\n",
    "        string = ''.join([self.itos[token] for token in tokens])\n",
    "\n",
    "        return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "li04PhKa56jy"
   },
   "outputs": [],
   "source": [
    "## Positional Encoding\n",
    "\n",
    "class PositionalEncodings(nn.Module) :\n",
    "    '''\n",
    "    Computes and Store the Positional Encodings.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, config : dict) :\n",
    "        super().__init__()\n",
    "\n",
    "        pos = torch.tensor([n for n in range(config['max_tokens'])], dtype = torch.float32)\n",
    "        pos = torch.unsqueeze(pos, 1)\n",
    "        i = torch.tensor([n for n in range(0, config['WE_dim'], 2)], dtype = torch.float32)\n",
    "\n",
    "        # PE_pos2i = sin(pos/10000^(2i/WE_dim))\n",
    "        # PE_pos2i+1 = cos((pos/10000^(2i/WE_dim))\n",
    "        PE = np.zeros((config['max_tokens'], config['WE_dim']), dtype = np.float32)\n",
    "        PE[:, 0::2] = np.sin(pos/(10000**(2*i/config['WE_dim']))) # all even col\n",
    "        PE[:, 1::2] = np.cos(pos/(10000**(2*i/config['WE_dim']))) # all odd col\n",
    "        PE = torch.tensor(PE, dtype = torch.float32)\n",
    "\n",
    "        self.register_buffer('PE', PE)\n",
    "\n",
    "    def forward(self, WE : torch.Tensor) -> torch.Tensor :\n",
    "\n",
    "        ## Adds Positional Encodings to the Word Embeddings ##\n",
    "\n",
    "        PE = self.PE.to('cuda')\n",
    "\n",
    "        return WE+self.PE[:WE.size(0), :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "dFgbUNEd56jz"
   },
   "outputs": [],
   "source": [
    "## Transformer Block\n",
    "\n",
    "class CSAttention(nn.Module) :\n",
    "    '''\n",
    "    Causal Multi-Headed Self-Attention\n",
    "    '''\n",
    "\n",
    "    def __init__(self, config : dict) :\n",
    "        super().__init__()\n",
    "\n",
    "        # config parmeters\n",
    "        self.WE_dim = config['WE_dim']\n",
    "        self.n_heads = config['n_heads']\n",
    "        self.batch_size = config['batch_size']\n",
    "        self.seq_len = config['seq_len']\n",
    "        self.heads_dim = self.WE_dim // self.n_heads\n",
    "\n",
    "        # querry, key, value\n",
    "        self.Q = nn.Linear(self.WE_dim, self.WE_dim)\n",
    "        self.K = nn.Linear(self.WE_dim, self.WE_dim)\n",
    "        self.V = nn.Linear(self.WE_dim, self.WE_dim)\n",
    "\n",
    "        self.scale = torch.sqrt(torch.tensor([self.heads_dim], dtype = torch.float32))\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.out_proj = nn.Linear(self.WE_dim, self.WE_dim)\n",
    "\n",
    "    def causal_mask(self, seq_len : int) -> torch.Tensor :\n",
    "\n",
    "        ## Computes Causal Mask ##\n",
    "\n",
    "        # size (1, 1, config['seq_len'], config['seq_len'])\n",
    "        mask = torch.tensor(np.ones((self.seq_len, self.seq_len)))\n",
    "        mask = torch.tril(mask)\n",
    "        mask = torch.unsqueeze(torch.unsqueeze(mask, 0), 0)\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor :\n",
    "\n",
    "        ## Computes the Output of the Causal Self-Attention Mechanism ##\n",
    "\n",
    "        # resize Q K V for multi-head\n",
    "        # size (batch_size, self.n_heads, seq_len, self.heads_dim)\n",
    "        Q = self.Q(x).to('cuda')\n",
    "        K = self.K(x).to('cuda')\n",
    "        V = self.V(x).to('cuda')\n",
    "        Q = torch.transpose(Q.view(self.batch_size, self.seq_len, self.n_heads, self.heads_dim), 1, 2)\n",
    "        K = torch.transpose(K.view(self.batch_size, self.seq_len, self.n_heads, self.heads_dim), 1, 2)\n",
    "        V = torch.transpose(V.view(self.batch_size, self.seq_len, self.n_heads, self.heads_dim), 1, 2)\n",
    "\n",
    "        # attention computation\n",
    "        # E_ij = (Q_i*K_j)/scale\n",
    "        energy = torch.matmul(Q, torch.transpose(K, -2, -1))/self.scale.to('cuda')\n",
    "        mask = self.causal_mask(self.seq_len).to('cuda')\n",
    "        energy = energy.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        # AW_ij = softmax(E_ij)\n",
    "        attention = torch.softmax(energy, dim = -1)\n",
    "        attention = self.dropout(attention)\n",
    "\n",
    "        out = torch.matmul(attention, V)\n",
    "\n",
    "        # combine heads\n",
    "        out = torch.transpose(out, 1, 2).contiguous().view(self.batch_size, self.seq_len, self.WE_dim)\n",
    "        out = self.out_proj(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module) :\n",
    "    '''\n",
    "    Transformer Block with\n",
    "    Causal Multi-Headed Self-Attention,\n",
    "    Dropout,\n",
    "    Multi-Layer Perceptron (Feed-Forward Neural Network)\n",
    "    and Layer Normalization\n",
    "    '''\n",
    "\n",
    "    def __init__(self, config : dict) :\n",
    "        super().__init__()\n",
    "\n",
    "        # multi-headed causal self-attention\n",
    "        self.CausalSelfAttn = CSAttention(config)\n",
    "\n",
    "        # dropout\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "\n",
    "        # feed-forward neural network\n",
    "        self.MLP = nn.Sequential(nn.Linear(config['WE_dim'], config['FFN_dim']), nn.ReLU(), nn.Linear(config['FFN_dim'], config['WE_dim']))\n",
    "\n",
    "        # layer normalization\n",
    "        self.LayerNorm_1 = nn.LayerNorm(config['WE_dim'])\n",
    "        self.LayerNorm_2 = nn.LayerNorm(config['WE_dim'])\n",
    "\n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor :\n",
    "\n",
    "        ## Computes Residual Connections, Attention and MLP ##\n",
    "\n",
    "        x = x + self.CausalSelfAttn(self.LayerNorm_1(x))\n",
    "        out = x + self.MLP(self.LayerNorm_2(x))\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "id": "D6i90dh656jz"
   },
   "outputs": [],
   "source": [
    "## Transformer Model\n",
    "\n",
    "class TransformerModel(nn.Module) :\n",
    "    '''\n",
    "    The Decoder Only Transformer Model\n",
    "    '''\n",
    "\n",
    "    def __init__(self, config : dict) :\n",
    "        super().__init__()\n",
    "\n",
    "        self.WE_dim = config['WE_dim']\n",
    "        self.n_layers = config['n_layers']\n",
    "        self.vocab_size = config['vocab_size']\n",
    "\n",
    "        self.WTE = nn.Embedding(self.vocab_size, self.WE_dim)\n",
    "        self.WPE = PositionalEncodings(config)\n",
    "\n",
    "        self.dropout = nn.Dropout(config['dropout'])\n",
    "        self.Blocks = nn.ModuleList([TransformerBlock(config) for _ in range(self.n_layers)])\n",
    "        self.Final_LayerNorm = nn.LayerNorm(self.WE_dim)\n",
    "        self.LM_Head = nn.Linear(self.WE_dim, self.vocab_size)\n",
    "\n",
    "    def forward(self, idx: torch.Tensor) -> torch.Tensor :\n",
    "\n",
    "        ## Generates Logits for Each Token ##\n",
    "\n",
    "        # embeddings\n",
    "        tok_emb = self.WTE(idx)\n",
    "        pos_emb = self.WPE(tok_emb)\n",
    "        x = self.dropout(tok_emb + pos_emb)\n",
    "\n",
    "        # transformer block\n",
    "        for Block in self.Blocks:\n",
    "\n",
    "            x = Block(x)\n",
    "\n",
    "        x = self.Final_LayerNorm(x)\n",
    "        logits = self.LM_Head(x)\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def generate(self, tokenized : list[int], config : dict) -> list[int] :\n",
    "\n",
    "        ## Generate a List of Tokens Given a Prompt ##\n",
    "\n",
    "        input_seq = torch.unsqueeze(torch.tensor(tokenized, dtype = torch.long), 0).to('cuda')\n",
    "        gen_tokens = tokenized\n",
    "        self.to('cuda')\n",
    "\n",
    "        # generate characters\n",
    "        for _ in range(config['max_len']):\n",
    "\n",
    "            logits = self.forward(input_seq)\n",
    "            next_logits = logits[:, -1, :]\n",
    "            next_logits = next_logits/config['temperature'] # if temperature = 1.0 nothing changes\n",
    "\n",
    "            # select the token with the highest probability\n",
    "            probas = torch.softmax(next_logits, dim = -1)\n",
    "            next_token = torch.argmax(probas, 1)\n",
    "            next_token = torch.unsqueeze(next_token, 0).to('cuda')\n",
    "\n",
    "            input_seq = torch.cat([input_seq, next_token], dim = 1)\n",
    "\n",
    "            gen_tokens.append(next_token.item())\n",
    "\n",
    "\n",
    "        return gen_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "ybwpblUP56jz"
   },
   "outputs": [],
   "source": [
    "## Trainig Function\n",
    "\n",
    "def train(model : TransformerModel, dataset : CharDataset, config : dict) -> tuple[TransformerModel, list] :\n",
    "\n",
    "    ## Trains the Model with Adam Optimizer and Stores Cross-Entropy Loss ##\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size = config['batch_size'], shuffle=True)\n",
    "\n",
    "    # adam optimizer and cross-entropy loss\n",
    "    optimizer = optim.Adam(model.parameters(), lr = config['learning_rate'])\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    monitor_loss = []\n",
    "    steps_count = 0\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "\n",
    "        print('Epoch', epoch+1)\n",
    "        total_loss = 0\n",
    "        steps_loss = [] # store the cross-entropy loss at each step\n",
    "\n",
    "        for batch_idx, (X, Y) in enumerate(train_loader) :\n",
    "\n",
    "            X, Y = X.to(torch.int64), Y.to(torch.int64)\n",
    "            X, Y = X.to('cuda'), Y.to('cuda')\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(X)\n",
    "            loss = loss_fn(logits.view(-1, model.vocab_size), Y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "            steps_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            steps_count += 1\n",
    "\n",
    "            # monitor steps progression\n",
    "            if steps_count%50 == 0 :\n",
    "\n",
    "                print('Computed step', steps_count)\n",
    "\n",
    "            if config['max_steps'] != None :\n",
    "\n",
    "                # stop training ifwe reach max_steps\n",
    "                if steps_count == config['max_steps'] :\n",
    "\n",
    "                    monitor_loss.append(steps_loss)\n",
    "                    print('Maximum number of steps reached')\n",
    "\n",
    "                    return model, monitor_loss\n",
    "\n",
    "        monitor_loss.append(steps_loss)\n",
    "\n",
    "    return model, monitor_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iGPY4i-e56jz"
   },
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "id": "cRDefgkSTJ4v"
   },
   "outputs": [],
   "source": [
    "# model configuration\n",
    "config = {'WE_dim': 768,\n",
    "          'n_heads': 8,\n",
    "          'n_layers': 12,\n",
    "          'batch_size': 128,\n",
    "          'seq_len': 128,\n",
    "          'FFN_dim': 1024,\n",
    "          'dropout': 0.1,\n",
    "          'learning_rate': 1e-3,\n",
    "          'max_tokens': 256,\n",
    "          'temperature': 1.0,\n",
    "          'max_steps': 1000} # Optional : set to None if we don't want to stop training after reaching max_steps steps\n",
    "\n",
    "\n",
    "## Retrieve Data from the File 'input.txt' ##\n",
    "\n",
    "data = get_data_from_file('input.txt')\n",
    "dataset = CharDataset(config, data)\n",
    "\n",
    "config['vocab_size'] = dataset.get_vocab_size()\n",
    "config['n_train_samp'] = len(dataset)\n",
    "config['epochs'] = 1\n",
    "#steps_per_epoch = config['n_train_samp']//config['batch_size']\n",
    "#config['epochs'] = int(np.ceil(config['max_steps']/steps_per_epoch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7VC8aH556jz",
    "outputId": "94b45736-9dfa-4f6c-e46c-793274bb6b40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Maximum number of steps reached\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Trains the Model with the Configuration 'config' ##\n",
    "\n",
    "model = TransformerModel(config).to('cuda')\n",
    "model, monitor_loss = train(model, dataset, config)\n",
    "\n",
    "# saves the model\n",
    "torch.save(model.state_dict(), 'model_'+str(config['max_steps'])+'steps.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ga967t_g56jz"
   },
   "source": [
    "## Generation and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "oTpGUMm2sHLC",
    "outputId": "493a79f4-5c47-4dd8-b761-9c0927a79eb1"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'monitor_loss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-3fc6c411f65a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# graph of cross-entropy loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmonitor_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'monitor_loss' is not defined"
     ]
    }
   ],
   "source": [
    "# graph of cross-entropy loss\n",
    "for e in range(len(monitor_loss)) :\n",
    "\n",
    "    x = [i for i in range(len(monitor_loss[e]))]\n",
    "    y = monitor_loss[e]\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.title('Cross-entropy loss over training steps for epoch '+str(e+1))\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel ('Number of steps')\n",
    "    plt.ylabel ('Cross-entropy loss')\n",
    "\n",
    "    # saving graph\n",
    "    plt.savefig('cross-entropy_graph_'+str(config['max_steps'])+'steps')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 463
    },
    "id": "bj9tmoyd56jz",
    "outputId": "86042090-fc24-46ef-964e-bb67092da014"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-98-bcb5a264d3ed>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('model_'+str(config['max_steps'])+'steps.pth'))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 13, 8, 96]' is invalid for input of size 10752",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-bcb5a264d3ed>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mseq_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_len'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0mcompletion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-612f18f511ab>\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, tokenized, config, seq_len)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_len'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0ms_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m             \u001b[0mnext_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m             \u001b[0mnext_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_logits\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'temperature'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# if temperature = 1.0 nothing changes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-97-612f18f511ab>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, idx, seq_len)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mBlock\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlocks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFinal_LayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-75a8d2e99864>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, seq_len)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m## Computes Residual Connections, Attention and MLP ##\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCausalSelfAttn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-75a8d2e99864>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mK\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheads_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 13, 8, 96]' is invalid for input of size 10752"
     ]
    }
   ],
   "source": [
    "\n",
    "## Generate Shakespearean-Like Text Given a Seed String ##\n",
    "\n",
    "# loades the model\n",
    "model = TransformerModel(config)\n",
    "model.load_state_dict(torch.load('model_'+str(config['max_steps'])+'steps.pth'))\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad() :\n",
    "\n",
    "    context = \"O God, O God!\"\n",
    "    config['batch_size'] = 1\n",
    "    tokenized_context = dataset.tokenize(context)\n",
    "    config['seq_len'] = len(tokenized_context)\n",
    "    config['max_len'] = config['seq_len']*20\n",
    "    y = model.generate(tokenized_context, config)\n",
    "    completion = dataset.tokens_to_string(y)\n",
    "\n",
    "print(completion)\n",
    "\n",
    "# save completion\n",
    "filename = 'completion_'+str(config['max_steps'])+'steps.txt'\n",
    "\n",
    "with open(filename, 'w') as f :\n",
    "    f.write('context : \\\"'+context+'\\\"\\nOutput : '+completion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lso97yjM05i9"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
